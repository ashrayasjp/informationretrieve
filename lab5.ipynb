{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Categorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import math, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Cats are playful and love sleeping\",\n",
    "    \"Dogs are loyal and friendly animals\",\n",
    "    \"Many families keep cats or dogs as pets\",\n",
    "    \"A kitten is a young domestic cat\",\n",
    "    \"This car is very fast on the highway\",\n",
    "    \"Electric vehicles are eco friendly\",\n",
    "    \"The new sports car has a powerful engine\",\n",
    "    \"Cars consume fuel or electricity to run\"\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    \"animal\", \"animal\", \"animal\", \"animal\",\n",
    "    \"vehicle\", \"vehicle\", \"vehicle\", \"vehicle\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing & Vocabulary Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['a', 'and', 'animals', 'are', 'as', 'car', 'cars', 'cat', 'cats', 'consume', 'dogs', 'domestic', 'eco', 'electric', 'electricity', 'engine', 'families', 'fast', 'friendly', 'fuel', 'has', 'highway', 'is', 'keep', 'kitten', 'love', 'loyal', 'many', 'new', 'on', 'or', 'pets', 'playful', 'powerful', 'run', 'sleeping', 'sports', 'the', 'this', 'to', 'vehicles', 'very', 'young']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    return text.split()\n",
    "\n",
    "docs_tokens = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# Vocabulary\n",
    "vocab = sorted(set(word for doc in docs_tokens for word in doc))\n",
    "print(\"Vocabulary:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Naive Bayes Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Predictions: ['animal', 'animal', 'animal', 'animal', 'vehicle', 'vehicle', 'vehicle', 'vehicle']\n"
     ]
    }
   ],
   "source": [
    "def train_nb(docs_tokens, labels, vocab):\n",
    "    classes = set(labels)\n",
    "    priors = {c: labels.count(c)/len(labels) for c in classes}\n",
    "    \n",
    "    word_counts = {c: defaultdict(int) for c in classes}\n",
    "    total_words = {c: 0 for c in classes}\n",
    "    \n",
    "    for tokens, lab in zip(docs_tokens, labels):\n",
    "        for w in tokens:\n",
    "            word_counts[lab][w] += 1\n",
    "            total_words[lab] += 1\n",
    "    \n",
    "    V = len(vocab)\n",
    "    likelihood = {c: {} for c in classes}\n",
    "    for c in classes:\n",
    "        for w in vocab:\n",
    "            likelihood[c][w] = (word_counts[c][w]+1)/(total_words[c]+V)\n",
    "    \n",
    "    return priors, likelihood\n",
    "\n",
    "def predict_nb(tokens, priors, likelihood, vocab):\n",
    "    scores = {}\n",
    "    for c in priors:\n",
    "        score = math.log(priors[c])\n",
    "        for w in tokens:\n",
    "            if w in vocab:\n",
    "                score += math.log(likelihood[c].get(w, 1/len(vocab)))\n",
    "        scores[c] = score\n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "priors, likelihood = train_nb(docs_tokens, labels, vocab)\n",
    "nb_preds = [predict_nb(doc, priors, likelihood, vocab) for doc in docs_tokens]\n",
    "print(\"Naive Bayes Predictions:\", nb_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Predictions: ['animal', 'animal', 'animal', 'vehicle', 'vehicle', 'animal', 'vehicle', 'vehicle']\n"
     ]
    }
   ],
   "source": [
    "def vectorize(tokens, vocab):\n",
    "    vec = np.zeros(len(vocab))\n",
    "    cnt = Counter(tokens)\n",
    "    for i, w in enumerate(vocab):\n",
    "        vec[i] = cnt[w]\n",
    "    return vec\n",
    "\n",
    "doc_vectors = np.array([vectorize(doc, vocab) for doc in docs_tokens])\n",
    "\n",
    "def cosine_sim(v1, v2):\n",
    "    if np.linalg.norm(v1)==0 or np.linalg.norm(v2)==0:\n",
    "        return 0\n",
    "    return np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "\n",
    "def predict_knn(test_vec, train_vecs, labels, k=3):\n",
    "    sims = [cosine_sim(test_vec, v) for v in train_vecs]\n",
    "    top_idx = np.argsort(sims)[-k:]\n",
    "    top_labels = [labels[i] for i in top_idx]\n",
    "    return Counter(top_labels).most_common(1)[0][0]\n",
    "\n",
    "knn_preds = [predict_knn(v, doc_vectors, labels, k=3) for v in doc_vectors]\n",
    "print(\"KNN Predictions:\", knn_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rocchio Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rocchio Predictions: ['animal', 'animal', 'animal', 'animal', 'vehicle', 'vehicle', 'vehicle', 'vehicle']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_rocchio(vectors, labels):\n",
    "    classes = set(labels)\n",
    "    centroids = {}\n",
    "    for c in classes:\n",
    "        idxs = [i for i, l in enumerate(labels) if l==c]\n",
    "        centroids[c] = np.mean(vectors[idxs], axis=0)\n",
    "    return centroids\n",
    "\n",
    "def predict_rocchio(vec, centroids):\n",
    "    sims = {c: cosine_sim(vec, centroids[c]) for c in centroids}\n",
    "    return max(sims, key=sims.get)\n",
    "\n",
    "centroids = train_rocchio(doc_vectors, labels)\n",
    "roc_preds = [predict_rocchio(v, centroids) for v in doc_vectors]\n",
    "print(\"Rocchio Predictions:\", roc_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Predictions: ['animal', 'animal', 'animal', 'animal', 'vehicle', 'vehicle', 'vehicle', 'vehicle']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def entropy(labels):\n",
    "    total = len(labels)\n",
    "    counts = Counter(labels)\n",
    "    return -sum((c/total)*math.log2(c/total) for c in counts.values())\n",
    "\n",
    "def info_gain(labels, feature_presence):\n",
    "    total_ent = entropy(labels)\n",
    "    yes = [l for l, pres in zip(labels, feature_presence) if pres]\n",
    "    no = [l for l, pres in zip(labels, feature_presence) if not pres]\n",
    "    gain = total_ent\n",
    "    if yes:\n",
    "        gain -= (len(yes)/len(labels))*entropy(yes)\n",
    "    if no:\n",
    "        gain -= (len(no)/len(labels))*entropy(no)\n",
    "    return gain\n",
    "\n",
    "def train_dt(docs_tokens, labels, vocab, depth=1, max_depth=3):\n",
    "    if len(set(labels))==1 or depth>max_depth:\n",
    "        return Counter(labels).most_common(1)[0][0]\n",
    "    \n",
    "    best_word, best_gain = None, -1\n",
    "    for w in vocab:\n",
    "        presence = [w in doc for doc in docs_tokens]\n",
    "        g = info_gain(labels, presence)\n",
    "        if g > best_gain:\n",
    "            best_gain, best_word = g, w\n",
    "    \n",
    "    if not best_word:\n",
    "        return Counter(labels).most_common(1)[0][0]\n",
    "    \n",
    "    tree = {best_word:{}}\n",
    "    yes_idx = [i for i, doc in enumerate(docs_tokens) if best_word in doc]\n",
    "    no_idx = [i for i in range(len(docs_tokens)) if i not in yes_idx]\n",
    "    \n",
    "    tree[best_word]['yes'] = train_dt([docs_tokens[i] for i in yes_idx],\n",
    "                                      [labels[i] for i in yes_idx], vocab, depth+1, max_depth)\n",
    "    tree[best_word]['no'] = train_dt([docs_tokens[i] for i in no_idx],\n",
    "                                     [labels[i] for i in no_idx], vocab, depth+1, max_depth)\n",
    "    return tree\n",
    "\n",
    "def predict_dt(tree, doc):\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    word = list(tree.keys())[0]\n",
    "    return predict_dt(tree[word]['yes'], doc) if word in doc else predict_dt(tree[word]['no'], doc)\n",
    "\n",
    "dtree = train_dt(docs_tokens, labels, vocab)\n",
    "dt_preds = [predict_dt(dtree, doc) for doc in docs_tokens]\n",
    "print(\"Decision Tree Predictions:\", dt_preds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3196968d684371006099b3d55edeef8ed90365227a30deaef86e5d4aa8519be0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
